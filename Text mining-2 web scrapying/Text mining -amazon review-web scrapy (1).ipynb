{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0758d4",
   "metadata": {},
   "source": [
    "# Assignment-11-Text Mining-Extract Amazon Reviews using Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de49c5",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "- If you are using conda, then you can install scrapy from the conda-forge using the following command.\n",
    "\n",
    "- conda install -c conda-forge scrapy\n",
    "- In case you are not using conda, you can use pip and directly install it in your system using the below command.\n",
    "\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15585fd6",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "- After scrapy installation, open cmd prompt from conda.\n",
    "\n",
    "- To create a scrapy project use following command in cmd prompt.\n",
    "\n",
    "scrapy startproject Scrape_AmazonReviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd337eaf",
   "metadata": {},
   "source": [
    "Once you have created the project, you will find \"Scrape_AmazonReviews\" file in root directory of your system. In which, one is a folder which contains your scrapy code, and other is your scrapy configuration file. Scrapy configuration helps in running and deploying the Scrapy project on a server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a0d49",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "Once we have the project in place, we need to create a spider. A spider is a chunk of python code that determines, how a web page will be scrapped. It is the main component that crawls different web pages and extracts content out of it. In our case, this will be the code chunk that will perform the task of visiting Amazon and scraping Amazon reviews. To create a spider, you can type in following command in same cmd prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e9dbd",
   "metadata": {},
   "source": [
    "scrapy genspider amazon_review https://www.amazon.in/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0aa8e8",
   "metadata": {},
   "source": [
    "This will create python file named \"amazon_review.py\" in your root directory, which you need to place inside \"Scrape_AmazonReviews\\Scrape_AmazonReviews\\spiders\" folder.\n",
    "\n",
    "The \"amazon_review.py\" file contains below scrapy parser code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class AmazonReviewSpider(scrapy.Spider):\n",
    "    name = 'amazon_review'\n",
    "    allowed_domains = ['amazon.in']\n",
    "    start_urls = ['http://amazon.in']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f331a5",
   "metadata": {},
   "source": [
    "# Step 4:\n",
    "Spider gets created within a \"spiders\" folder inside the project directory. Once you go into the \"Scrape_AmazonReviews\" folder/project, you will see a directory structure like the one below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b1518",
   "metadata": {},
   "source": [
    "- Scrapy files description:\n",
    "\n",
    "Let us understand the \"Scrape_AmazonReviews\" Scrapy project structure and supporting files inside in a bit more detail. Main files inside Scrapy project directory includes,\n",
    "\n",
    "items.py\n",
    "\n",
    "Items are containers that will be loaded with the scraped data.\n",
    "\n",
    "middleware.py\n",
    "\n",
    "The spider middleware is a framework of hooks into Scrapyâ€™s spider processing mechanism where you can plug custom functionality to process the responses that are sent to Spiders for processing and to handle the requests and items that are generated from spiders.\n",
    "\n",
    "pipelines.py\n",
    "\n",
    "After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially. Each item pipeline component is a Python class.\n",
    "\n",
    "settings.py\n",
    "\n",
    "It allows one to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves.\n",
    "\n",
    "spiders folder\n",
    "\n",
    "The Spiders is a directory which contains all spiders/crawlers as Python classes. Whenever one runs/crawls any spider, then scrapy looks into this directory and tries to find the spider with its name provided by the user. Spiders define how a certain site or a group of sites will be scraped, including how to perform the crawl and how to extract data from their pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f7a82",
   "metadata": {},
   "source": [
    "# Step 5:\n",
    "Analyzing HTML structure of the webpage:\n",
    "\n",
    "For Egs: https://www.amazon.in/product-reviews/9387779262/ref=cm_cr_getr_d_paging_btm_prev_1?ie=UTF8&pageNumber= (Extraction of reviews from this web-page)\n",
    "\n",
    "Now, before we actually start writing spider implementation in python for scraping Amazon reviews, we need to identify patterns in the target web page. Below is the page we are trying to scrape which contains different reviews about the product 'My First Library: Boxset of 10 Board Books for Kids' on Amazon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cec834",
   "metadata": {},
   "source": [
    "# Step 6:\n",
    "Then we need to define a parse function that gets fired up whenever our spider visits a new page. In the parse function, we need to identify patterns in the targeted page structure. Spider then looks for these patterns and extracts them out from the web page.\n",
    "\n",
    "Below is a code sample of Scrapy parser for scraping Amazon reviews. let's name the file as \"extract_reiews.py\" and save it in \"Scrape_AmazonReviews\\Scrape_AmazonReviews\\spiders\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class AmazonReviewsSpider(scrapy.Spider):\n",
    "\n",
    "    # Spider name\n",
    "    name = 'amazon_reviews'\n",
    "\n",
    "    # Domain names to scrape\n",
    "    allowed_domains = ['amazon.in']\n",
    "\n",
    "    # Base URL for the product reviews\n",
    "    myBaseUrl = \"https://www.amazon.in/product-reviews/9387779262/ref=cm_cr_getr_d_paging_btm_prev_1?ie=UTF8&pageNumber=\"\n",
    "    start_urls=[]\n",
    "\n",
    "    # Creating list of urls to be scraped by appending page number a the end of base url\n",
    "    for i in range(1,121):\n",
    "        start_urls.append(myBaseUrl+str(i))\n",
    "\n",
    "    # Defining a Scrapy parser\n",
    "    def parse(self, response):\n",
    "            data = response.css('#cm_cr-review_list')\n",
    "            \n",
    "            # Collecting product star ratings\n",
    "            star_rating = data.css('.review-rating')\n",
    "\n",
    "            # Collecting user reviews\n",
    "            comments = data.css('.review-text')\n",
    "            count = 0\n",
    "\n",
    "            # Combining the results\n",
    "            for review in star_rating:\n",
    "                yield{'stars': \n",
    "                      ''.join(review.xpath('.//text()').extract()),\n",
    "                      'comment': \n",
    "                          ''.join(comments[count].xpath(\".//text()\").extract())\n",
    "                     }\n",
    "                count=count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3acdd6",
   "metadata": {},
   "source": [
    "# Step 7:\n",
    "Finally, we have successfully built our spider. The only task now left is to run this spider. We can run this spider by using the runspider command. It takes to input the spider file to run and the output file to store the collected results. In the case below, spider file is amazon_reviews.py and the output file is amazon_reviews.csv\n",
    "\n",
    "To run this, open cmd prompt and type below command:\n",
    "\n",
    "scrapy runspider Scrape_AmazonReviews\\Scrape_AmazonReviews\\spiders\\extract_reviews.py -o extract_reviews.csv\n",
    "The extracted \"extract_reviews.csv\" will get saved to default directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a496fb",
   "metadata": {},
   "source": [
    "# Step 8:\n",
    "The extracted reviews file is ready to use and can be open using python as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5275c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T09:36:17.018362Z",
     "start_time": "2023-03-18T09:36:14.627035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Attractive for kid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Use full\\n  \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Easy to teach\\n  \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Product is good an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bought this as a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Nice product for k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Must have for kids...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Books r very nice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    good\\n  \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    My kid loves these...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                stars                                            comment\n",
       "0  4.0 out of 5 stars  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Attractive for kid...\n",
       "1  4.0 out of 5 stars         \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Use full\\n  \\n\n",
       "2  4.0 out of 5 stars    \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Easy to teach\\n  \\n\n",
       "3  5.0 out of 5 stars  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Product is good an...\n",
       "4  5.0 out of 5 stars  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bought this as a g...\n",
       "5  5.0 out of 5 stars  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Nice product for k...\n",
       "6  5.0 out of 5 stars  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Must have for kids...\n",
       "7  5.0 out of 5 stars  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Books r very nice ...\n",
       "8  5.0 out of 5 stars             \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    good\\n  \\n\n",
       "9  5.0 out of 5 stars  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    My kid loves these..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "book=pd.read_csv('extract_reviews.csv')\n",
    "book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
